{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import pafy\n",
    "import random\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from moviepy.editor import *\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hub.load('https://tfhub.dev/google/movenet/multipose/lightning/1')\n",
    "movenet = model.signatures['serving_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_height, image_width = 64,64\n",
    "sequence_length = 20\n",
    " \n",
    "dataset_directory = os.path.join('UCF50')\n",
    "classes_list = [\"Punch\", \"Biking\", \"SoccerJuggling\", \"HorseRace\"]\n",
    " \n",
    "model_output_size = len(classes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_extraction(video_path):\n",
    "    # Empty List declared to store video frames\n",
    "    frames_list = []\n",
    "     \n",
    "    # Reading the Video File Using the VideoCapture\n",
    "    video_reader = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    #get the total number of frames in the video\n",
    "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    skip_frames_window = max(int(video_frames_count/sequence_length),1)\n",
    "\n",
    "\n",
    "    for frame_counter in range(sequence_length):\n",
    "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
    "        success, frame = video_reader.read()\n",
    "\n",
    "        if not success:\n",
    "            break\n",
    "        \n",
    "        img = frame.copy()\n",
    "        img = cv2.resize(img,(320,240))\n",
    "        img = tf.image.resize_with_pad(tf.expand_dims(img, axis=0), 192,256)\n",
    "        input_img = tf.cast(img, dtype=tf.int32)\n",
    "\n",
    "        results = movenet(input_img)\n",
    "        ROIs = results['output_0'].numpy()[:,:,51:]\n",
    "        keypoints_with_scores = results['output_0'].numpy()[:,:,:51].reshape((6,17,3))\n",
    "        normalized_points = keypoints_with_scores.copy()\n",
    "\n",
    "        # print(ROIs)\n",
    "        k=0\n",
    "        while True:\n",
    "            if ROIs[0][k][-1]>0.1:\n",
    "                normalized_points[k,:,0] = normalized_points[k,:,0]-ROIs[0][k][0]\n",
    "                normalized_points[k,:,1] = normalized_points[k,:,1]-ROIs[0][k][1]\n",
    "            else:\n",
    "                normalized_points[k] = normalized_points[np.random.randint(k+1)]\n",
    "            k+= 1\n",
    "            if k==6:\n",
    "                break\n",
    "        # print('methanata')\n",
    "        frames_list.append(normalized_points.reshape(6*17*3))\n",
    "     \n",
    "    video_reader.release()\n",
    " \n",
    "    return frames_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    " \n",
    "    # Declaring Empty Lists to store the features and labels values.\n",
    " \n",
    "    features = []\n",
    "    labels = []\n",
    "    video_file_paths = []\n",
    "     \n",
    "    # Iterating through all the classes mentioned in the classes list\n",
    "    for class_index, class_name in enumerate(classes_list):\n",
    "        print(f'Extracting Data of Class: {class_name}')\n",
    "         \n",
    "        # Getting the list of video files present in the specific class name directory\n",
    "        files_list = os.listdir(os.path.join(dataset_directory, class_name))\n",
    " \n",
    "        # Iterating through all the files present in the files list\n",
    "        for file_name in files_list:\n",
    " \n",
    "            # Construct the complete video path\n",
    "            video_file_path = os.path.join(dataset_directory, class_name, file_name)\n",
    " \n",
    "            # Calling the frame_extraction method for every video file path\n",
    "            frames = frames_extraction(video_file_path)\n",
    " \n",
    "            \n",
    "            if len(frames) == sequence_length:\n",
    "                features.append(frames)\n",
    "                labels.append(class_index)\n",
    "                video_file_paths.append(video_file_path)\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    " \n",
    "    return features, labels, video_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Data of Class: Punch\n",
      "Extracting Data of Class: Biking\n",
      "Extracting Data of Class: SoccerJuggling\n",
      "Extracting Data of Class: HorseRace\n"
     ]
    }
   ],
   "source": [
    "features, labels, video_file_paths = create_dataset() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded_labels = to_categorical(labels)\n",
    "seed_constant = 23\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels, test_size = 0.25, shuffle = True, random_state = seed_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(441, 20, 306)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(441, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(features_train.shape)\n",
    "labels_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_X,test_set_X=features_train,features_test\n",
    "def get_lnumber_of_data(train_set_Y,test_set_Y,):\n",
    "    number_of_train_samples=train_set_Y.shape[1] #Assuming one coloumn equals to one data set\n",
    "    number_of_test_samples=test_set_Y.shape[1]\n",
    "    \n",
    "    return(number_of_train_samples,number_of_test_samples)\n",
    "\n",
    "def flatten_X_data(train_set_X,test_set_X):\n",
    "    train_X_flatten=train_set_X.reshape(train_set_X.shape[0],-1).T\n",
    "    test_X_flatten=test_set_X.reshape(test_set_X.shape[0],-1).T\n",
    "    \n",
    "    return(train_X_flatten,test_X_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_flatten,test_X_flatten=flatten_X_data(train_set_X,test_set_X)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "faltten input data shape= (n,m)\n",
    "m= number of data sets\n",
    "n=number of features in one data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laye_size():\n",
    "    input_layer_size=train_X_flatten.shape[0]\n",
    "    #hidden_layer_1_Size=\n",
    "    #hidden_layer_2_size=\n",
    "    #.....\n",
    "    output_layer_size=2 #number of final outputs\n",
    "\n",
    "    return(input_layer_size,output_layer_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(layers_dims):\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layers_dims) - 1 # integer representing the number of layers\n",
    "     \n",
    "    for l in range(1, L + 1):\n",
    "       \n",
    "        parameters['W'+str(l)]=np.random.randn(layers_dims[l],layers_dims[l-1])*np.sqrt((2./layers_dims[l-1]))\n",
    "        parameters['b'+str(l)]=np.zeros((layers_dims[l],1))\n",
    "   \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_forward(A,W,b):\n",
    "    Z=np.dot(W,A)+b\n",
    "    cache=(A,W,b)\n",
    "    return Z,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z)) ,Z\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z),Z\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) \n",
    "    dZ[Z <= 0] = 0\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev,W,b,Activation):\n",
    "    if Activation==\"sigmoid\":\n",
    "        Z,Linear_cache=Linear_forward(A_prev,W,b)\n",
    "        A_new ,activation_cache=sigmoid(Z)\n",
    "    elif Activation==\"relu\":\n",
    "        Z,Linear_cache=Linear_forward(A_prev,W,b)\n",
    "        A_new ,activation_cache=relu(Z)\n",
    "    cache=(Linear_cache,activation_cache)\n",
    "    return A_new,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Forward_propagation(X,parameters):\n",
    "    caches=[]\n",
    "    A=X\n",
    "    L=len(parameters)//2\n",
    "    for l in range(1,L):\n",
    "        A_prev=A\n",
    "        A,cache=linear_activation_forward(A_prev,parameters[\"W\"+str(l)],parameters[\"b\"+str(l)],Activation=\"relu\")\n",
    "        caches.append(cache)\n",
    "    AL,cache=linear_activation_forward(A,parameters[\"W\"+str(L)],parameters[\"b\"+str(L)],Activation=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    assert(AL.shape==(4,X.shape[1]))\n",
    "    return AL,caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL,Y,parameters,lambd):\n",
    "    m=Y.shape[1]\n",
    "    L=len(parameters)//2\n",
    "    reg_value=0\n",
    "    for l in range(1,L+1):\n",
    "        reg_value=reg_value+np.sum(np.square(parameters[\"W\"+str(l)]))\n",
    "    \n",
    "    cost=(-1/m)*np.sum(np.multiply(Y,np.log(AL))+np.multiply((1-Y),np.log(1-AL)))+reg_value*(lambd/(2*m))\n",
    "    cost=np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ,cache,lambd):\n",
    "    A_prev,W,b=cache\n",
    "    m=A_prev.shape[1]\n",
    "\n",
    "    dW=(dZ@A_prev.T)/m+(lambd/m)*W\n",
    "    db=np.sum(dZ,axis=1,keepdims=True)/m\n",
    "    dA_prev=np.dot(W.T,dZ)\n",
    "    return dA_prev,dW,db\n",
    "\n",
    "def linear_activation_backward(dA,cache,activation,lambd):\n",
    "    linear_cache,activation_cache=cache\n",
    "    if activation==\"relu\":\n",
    "        dZ=relu_backward(dA,activation_cache)\n",
    "    elif activation==\"sigmoid\":\n",
    "        dZ=sigmoid_backward(dA,activation_cache)\n",
    "    dA_prev,dW,db=linear_backward(dZ,linear_cache,lambd)\n",
    "\n",
    "    return dA_prev,dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Backward_propagation(AL,Y,caches,lambd):\n",
    "    grads={}\n",
    "    L=len(caches)\n",
    "    m=AL.shape[1]\n",
    "    #print(Y.shape)\n",
    "    #Y=Y.rashape(AL.shape)\n",
    "    dAL=-(np.divide(Y,AL)-np.divide(1-Y,1-AL))\n",
    "    #print(AL.shape)\n",
    "    current_cache=caches[-1]\n",
    "    grads[\"dA\"+str(L-1)],grads[\"dW\"+str(L)],grads[\"db\"+str(L)]=linear_activation_backward(dAL,current_cache,\"sigmoid\",lambd)\n",
    "                            \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache=caches[l]\n",
    "        da_prev_temp,dW_temp,db_temp=linear_activation_backward(grads[\"dA\"+str(l+1)],current_cache,\"relu\",lambd)\n",
    "        grads[\"dA\"+str(l)]=da_prev_temp\n",
    "        grads[\"dW\"+str(l+1)]=dW_temp                             \n",
    "        grads[\"db\"+str(l+1)]=db_temp                                                                      \n",
    "                                                                   \n",
    "    return grads            \n",
    "                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(params,grads,learning_rate):\n",
    "    parameters=params.copy()\n",
    "    L=len(parameters)//2\n",
    "    for l in range(1,L):\n",
    "        parameters[\"W\"+str(l)]=parameters[\"W\"+str(l)]-learning_rate*grads[\"dW\"+str(l)]\n",
    "        parameters[\"b\"+str(l)]=parameters[\"b\"+str(l)]-learning_rate*grads[\"db\"+str(l)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer_dims=[train_X_flatten.shape[0],1000,100,20,10,labels_train.T.shape[0]]#should change the values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X,Y,Layer_dims,learning_rate,number_of_iterations,lambd):\n",
    "    costs=1\n",
    "    params=init_parameters(Layer_dims)\n",
    "    for i in range(0,number_of_iterations):\n",
    "        AL,caches= Forward_propagation(X,params)\n",
    "        cost=compute_cost(AL,Y,params,lambd)\n",
    "        grads=Backward_propagation(AL,Y,caches,lambd)\n",
    "        params=update_params(params,grads,learning_rate)\n",
    "        if i%100==0:\n",
    "            print(cost)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.743341777119012\n",
      "4.012086169784755\n",
      "3.233412948623884\n",
      "2.443088614944114\n",
      "2.383186757535256\n",
      "2.711255315439671\n",
      "1.7324315463309723\n",
      "1.6223788845708957\n",
      "1.5727783445490071\n",
      "1.5346216665112773\n",
      "1.49877096928897\n",
      "1.4641821567018918\n",
      "1.4305744191469716\n",
      "1.3978348954183935\n",
      "1.365922691146283\n",
      "1.3347800321039314\n",
      "1.3044002769854013\n",
      "1.274743834549128\n",
      "1.24579055881869\n",
      "1.2175252485795964\n",
      "1.1899255578013368\n",
      "1.1629797956524144\n",
      "1.13667344042926\n",
      "1.1110072919373075\n",
      "1.085940915547013\n",
      "1.061483740754761\n",
      "1.0376051996578821\n",
      "1.0142906077051814\n",
      "0.9915287572841978\n",
      "0.9693154647608666\n",
      "0.9476132981374179\n",
      "0.926432713030166\n",
      "0.9057514839520295\n",
      "0.8855627623591611\n",
      "0.8658522922593382\n",
      "0.8465989964427388\n",
      "0.8278131544313101\n",
      "0.8094586761035352\n",
      "0.7915430936865728\n",
      "0.7740434146341824\n",
      "0.7569593685087354\n",
      "0.740279364113638\n",
      "0.7239952223795042\n",
      "0.7081033672037845\n",
      "0.6925739652684033\n",
      "0.6774139962913038\n",
      "0.662615372093345\n",
      "0.6481643137460655\n",
      "0.634053363550355\n",
      "0.6202770536400265\n"
     ]
    }
   ],
   "source": [
    "parameters1=model(train_X_flatten,labels_train.T,Layer_dims,0.075,5000,0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "\n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "\n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((y.shape[0],y.shape[1]))\n",
    "\n",
    "    # Forward propagation\n",
    "    probas, caches = Forward_propagation(X, parameters)\n",
    "\n",
    "\n",
    "    # convert probas to 0/1 predictions\n",
    "    for r in range(0,m):\n",
    "        max_number=np.max(probas[:,r])\n",
    "        pos=np.where(probas[:,r]==max_number)[0][0]\n",
    "        p[pos,r]=1\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "\n",
    "    return p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "adf08f790d71190c7dc7f196c21aeadc9de8656ec93cc8c38f60b065a5eca6a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
